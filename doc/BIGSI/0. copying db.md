It is assumed that at this point you would have a single file for a BerkeleyDb which contains
data for the BIGSI index. The first step of uploading BIGSI index is to copy this db
file to the `/database` directory mounted in the BIGSI server instance.
```
KUBECONFIG=<kubeconfig_path> <kubectl_path> cp /path/to/db/file <namespace>/<bigsi-api-pod-name>:/database/<new_db_filename>
```
If the size of the db file is huge, make sure you submit a job to do the copying or run
the copying process in a Screen session.

Once the copying is finished, compare the md5sum between source db file and the destination
db file. Make sure they are equal.
```
md5sum <new-db-filename> > <new-db-filename>.md5
```

Last, update the ConfigMap on the Kubernetes for the BIGSI config such that the filename
is set to correct path. An example of such config looks like this:
```yaml
Data
====
config.yaml:
----
h: 1
k: 31
m: 28000000
nproc: 1
storage-engine: berkeleydb
storage-config:
  filename: /database/big-bigsi-bdb
  flag: "r" ## Change to 'c' for write access
```

###Sanity check
Make sure that the BIGSI instance has been set up correctly and pointing to the new db file
that has just been uploaded. Try send a request to the BIGSI service like this:
```
curl -X POST -H "Content-Type: application/json" -d '{"seq":"GGTGTTCCGCGGCGTGGACAACGTGGTTGCGTGCCTCGGCGACACCGCGGCCGCCGCCGAGGGGGGGG"}' http://<bigsi-service>/search/
```
