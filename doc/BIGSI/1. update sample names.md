**This step is dependent on the step 2 (preparing sample mapping) outlined in the
prerequisites section.**

We have a script for updating sample names in `bin/update_bigsi_sample_names.py`.
The script will take a sample mapping file generated in step 2 outlined in the
prerequisites section and the name of db engine as well as the path to the db file.
The script will update all the sample names for the samples listed in the mapping
file from the orginal `isolate_id` to the new `tracking_id` that is being used
throughout the Atlas system.

To run this script, first download the script to a location where the BIGSI instance
have mounted. You will also need to have the id mapping file in the same location.
For example:
```
KUBECONFIG=<kubeconfig_path> <kubectl_path> cp /path/to/mapping/file <namespace>/<bigsi-api-pod-name>:/path/to/script
```

Once we have all these in place, we can simply run the script to update the sample names:
```
python3 bin/update_bigsi_sample_names.py --storage_filepath /path/to/bigsi/db included.sample.mapping.csv berkeleydb
```

###Merge BIGSIs
If the index you have just uploaded is not the first one already in the cluster. You may want
to merge this new one with an existing index in the cluster. Make sure you have config files
for both index setup with correct path to the db files. Then run the bigsi command:
```shell
bigsi merge --config ./big_bigsi_index_config.yaml --merge_config ./sa_index_config.yaml &
```
This process is going to take some time.

###Sanity Check
Run the same query as you did during the step 0 (copying db) and make sure you get the
same samples with mapped tracking IDs this time:
```
curl -X POST -H "Content-Type: application/json" -d '{"seq":"GGTGTTCCGCGGCGTGGACAACGTGGTTGCGTGCCTCGGCGACACCGCGGCCGCCGCCGAGGGGGGGG"}' http://<bigsi-service>/search/
```
